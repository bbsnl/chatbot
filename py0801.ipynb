{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "py0801.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbsnl/chatbot/blob/master/py0801.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3oU1rYc1pgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "upload = files.upload()\n",
        "#確認是否上傳成功\n",
        "!ls *.* -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51UzNf6A7ED0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0154d90f-0d20-4315-f59b-f80107fea5b1"
      },
      "source": [
        "import re\n",
        "text_string = '文本最重要的來源無疑是網格。我們要把網路中的文本獲取形成一個文本數據庫。利用一個爬蟲抓取到網路中的信息。爬蟲的策略有廣度爬取和深度爬取。根據用戶的需求，爬蟲可以有主題爬蟲和通用爬蟲之分。'\n",
        "regex = '爬蟲'\n",
        "a     = '文本'\n",
        "p_string = text_string.split('。')\n",
        "for line in p_string:\n",
        "  if re.search(regex,line):\n",
        "    print(line)\n",
        "\n",
        "print('-----------------')\n",
        "\n",
        "for line in p_string:\n",
        "  if re.search(a,line):\n",
        "    print(line)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "利用一個爬蟲抓取到網路中的信息\n",
            "爬蟲的策略有廣度爬取和深度爬取\n",
            "根據用戶的需求，爬蟲可以有主題爬蟲和通用爬蟲之分\n",
            "-----------------\n",
            "文本最重要的來源無疑是網格\n",
            "我們要把網路中的文本獲取形成一個文本數據庫\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmspeWTu_rCq",
        "colab_type": "code",
        "outputId": "54385b6a-5de3-4b08-f90e-1bc6664a659b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#匹配任一個字符，(換行除外)\n",
        "#例a.c可匹配'abc','branch',不可匹配'add','crash'\n",
        "#'..t' 可匹配'bat','oat',不可匹配'it','table'\n",
        "\n",
        "import re\n",
        "text_string = '文本最重要的來源無疑是網格。我們要把網路中的文本獲取形成一個文本數據庫。利用一個爬蟲抓取到網路中的信息。爬蟲的策略有廣度爬取和深度爬取。根據用戶的需求，爬蟲可以有主題爬蟲和通用爬蟲之分。'\n",
        "regex = '用戶.'\n",
        "\n",
        "\n",
        "p_string = text_string.split('。')\n",
        "for line in p_string:\n",
        "  if re.search(regex,line):\n",
        "    print(line)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "根據用戶的需求，爬蟲可以有主題爬蟲和通用爬蟲之分\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br62-8O-D6kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3d508c4b-992b-4591-8ff1-1ab52674fc44"
      },
      "source": [
        "#匹配開始和結尾字符,^開始,$結尾\n",
        "#'^a'代表以字母a為開頭的字串\n",
        "#'a$'代表以字母a結尾的字串\n",
        "import re\n",
        "text_string = '文本最重要的來源無疑是網格。我們要把網路中的文本獲取形成一個文本數據庫。利用一個爬蟲抓取到網路中的信息。爬蟲的策略有廣度爬取和深度爬取。根據用戶的需求，爬蟲可以有主題爬蟲和通用爬蟲之分。'\n",
        "regex = '^文本'\n",
        "s='信息$'\n",
        "\n",
        "p_string = text_string.split('。')\n",
        "\n",
        "for line in p_string:\n",
        "  if re.search(regex,line):\n",
        "    print(line)\n",
        "    \n",
        "for line in p_string:\n",
        "  if re.search(s,line):\n",
        "    print(line)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "文本最重要的來源無疑是網格\n",
            "利用一個爬蟲抓取到網路中的信息\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w_f3iSJH4Nc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5cb45541-bb34-4961-d449-34e72b4230f4"
      },
      "source": [
        "#使用[]匹配多個字符\n",
        "#例如[bcr]at,代表的是要匹配出'bat','cat','rat'\n",
        "import re \n",
        "text_string=text_string = ['[重要的]今年第七號颱風23日登陸台灣東部沿海地區','上海發布車庫銷售監管通知：違規者暫停網簽資格','[緊要的]中國對印度發強硬信息，印度急切需要結束對峙']\n",
        "regex='^\\[[重緊]..\\]'\n",
        "for line in text_string:\n",
        "  if re.search(regex,line):\n",
        "    print(line)\n",
        "  else:\n",
        "    print('no match')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[重要的]今年第七號颱風23日登陸台灣東部沿海地區\n",
            "no match\n",
            "[緊要的]中國對印度發強硬信息，印度急切需要結束對峙\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjZ2cmmBIdao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}